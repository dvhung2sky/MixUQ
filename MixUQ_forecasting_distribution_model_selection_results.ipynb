{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qXE4vmPOjTsZ",
    "outputId": "5b5e903f-a2fd-48de-fbff-05a77538fdbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch==2.4.0\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.0%2Bcu121-cp310-cp310-linux_x86_64.whl (799.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.1/799.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.19.0\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.0%2Bcu121-cp310-cp310-linux_x86_64.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio==2.4.0\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.0%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.0.0 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.19.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.19.0) (11.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
      "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1+cu121\n",
      "    Uninstalling torch-2.5.1+cu121:\n",
      "      Successfully uninstalled torch-2.5.1+cu121\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.20.1+cu121\n",
      "    Uninstalling torchvision-0.20.1+cu121:\n",
      "      Successfully uninstalled torchvision-0.20.1+cu121\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.5.1+cu121\n",
      "    Uninstalling torchaudio-2.5.1+cu121:\n",
      "      Successfully uninstalled torchaudio-2.5.1+cu121\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0+cu121 torchaudio-2.4.0+cu121 torchvision-0.19.0+cu121 triton-3.0.0\n",
      "\u001b[33mWARNING: Skipping mamba-ssm as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping causal-conv1d as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting causal-conv1d\n",
      "  Downloading causal_conv1d-1.4.0.tar.gz (9.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from causal-conv1d) (2.4.0+cu121)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from causal-conv1d) (24.2)\n",
      "Collecting ninja (from causal-conv1d)\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->causal-conv1d) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->causal-conv1d) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->causal-conv1d) (1.3.0)\n",
      "Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: causal-conv1d\n",
      "  Building wheel for causal-conv1d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for causal-conv1d: filename=causal_conv1d-1.4.0-cp310-cp310-linux_x86_64.whl size=104867883 sha256=b5e7cf7e964b5e99275d97ba1e1b0ee4e3073f4593743ba1f1c6aa394a3008cc\n",
      "  Stored in directory: /root/.cache/pip/wheels/e3/dd/4c/205f24e151736bd22f5980738dd10a19af6f093b6f4dcab006\n",
      "Successfully built causal-conv1d\n",
      "Installing collected packages: ninja, causal-conv1d\n",
      "Successfully installed causal-conv1d-1.4.0 ninja-1.11.1.1\n",
      "Collecting mamba-ssm\n",
      "  Downloading mamba_ssm-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.4.0+cu121)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (24.2)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (1.11.1.1)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (0.8.0)\n",
      "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (3.0.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (4.46.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mamba-ssm) (12.6.77)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (4.66.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba-ssm) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mamba-ssm) (1.3.0)\n",
      "Building wheels for collected packages: mamba-ssm\n",
      "  Building wheel for mamba-ssm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for mamba-ssm: filename=mamba_ssm-2.2.2-cp310-cp310-linux_x86_64.whl size=323988104 sha256=6b082468a6abb6f6bc50c99263f17c6c7f5a2e8f6b275ed7998b81fb25279229\n",
      "  Stored in directory: /root/.cache/pip/wheels/57/7c/90/9f963468ecc3791e36e388f9e7b4a4e1e3f90fbb340055aa4d\n",
      "Successfully built mamba-ssm\n",
      "Installing collected packages: mamba-ssm\n",
      "Successfully installed mamba-ssm-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip uninstall mamba-ssm causal-conv1d\n",
    "!pip install causal-conv1d && pip install mamba-ssm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "-xfhg71QWnf5",
    "outputId": "fcd83358-dedd-4e30-c3c4-3c0a5b2abda8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd drive/MyDrive/ex_comparison_forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PY-yafD5b3ab"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muAZUrA2oD_5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "import sklearn.preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "import pickle\n",
    "from scipy import interpolate\n",
    "import random\n",
    "import time\n",
    "from lightgbm import LGBMRegressor\n",
    "# import optuna\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from scipy.stats import linregress\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "from sklearn.metrics import root_mean_squared_error as RMSE\n",
    "from sklearn.metrics import r2_score as R2\n",
    "from torchinfo import summary\n",
    "from mamba_ssm import Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXbf923DpiRB"
   },
   "outputs": [],
   "source": [
    "with open(f'list_EQ_id.p', 'rb') as f:\n",
    "    list_EQid = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrT7QT27piT5"
   },
   "outputs": [],
   "source": [
    "def getPositionEncoding(seq_len, d, n=10000):\n",
    "    P = np.zeros((seq_len, d))\n",
    "    for k in range(seq_len):\n",
    "        for i in np.arange(int(d/2)):\n",
    "            denominator = np.power(n, 2*i/d)\n",
    "            P[k, 2*i] = np.sin(k/denominator)\n",
    "            P[k, 2*i+1] = np.cos(k/denominator)\n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vBj_E7KpiZQ"
   },
   "outputs": [],
   "source": [
    "def prepare_data_batch(x, N1, N2, list_output_time, j = 2):\n",
    "  N0 = np.shape(x)[1]\n",
    "  xxx = np.stack([x[:,i:N0 - N1+i+1:j, :] for i in range(N1)], axis=2)\n",
    "  y = xxx[:,:,-N2:,-len(list_output_time):]\n",
    "\n",
    "  return np.array(xxx), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtJLVfMhppNq"
   },
   "outputs": [],
   "source": [
    "def data_provider2(data_time, data_static, data_y, batch_size=128):\n",
    "    data_set = h_Dataset(data_time, data_static, data_y)\n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    return data_set, data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDcAgMYWppQ8"
   },
   "outputs": [],
   "source": [
    "class h_Dataset(Dataset):\n",
    "    def __init__(self, data_time, data_static, data_y):\n",
    "        self.data_time = data_time\n",
    "        self.data_static = data_static\n",
    "        self.data_y = data_y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq_time = self.data_time[index]\n",
    "        seq_static = self.data_static[index]\n",
    "        seq_y = self.data_y[index]\n",
    "        return seq_time, seq_static, seq_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2D98tzHppTZ"
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model, kernel_size=3, padding=1, padding_mode='circular', bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "          x = self.value_embedding(x) + self.position_embedding(x)\n",
    "          return self.dropout(x)\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHCj3O0xppVy"
   },
   "outputs": [],
   "source": [
    "class Data_aggregation(nn.Module):\n",
    "    def __init__(self, L_all, N_static, dropout=0.1):\n",
    "        super(Data_aggregation, self).__init__()\n",
    "        self.projection = nn.Sequential(nn.Linear(N_static, L_all), nn.ReLU(), nn.Linear(L_all, L_all))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x_time, x_static):\n",
    "        x = self.projection(x_static)\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.cat([x_time, x], dim=2)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVweuHUap3x9"
   },
   "outputs": [],
   "source": [
    "def gen_data_AI_batch_2_model(df_new, list_idx, L_all, L_out, L_ground, N_input, N_output, N_static, sliding_step, list_time_series, list_output_time, list_static, real_scaler, static_scaler):\n",
    "  L_in = L_all - L_out\n",
    "  signal = [np.stack(list(df_new[k][list_idx])) for k in list_time_series ]\n",
    "  signal = np.stack(signal).transpose(1,2,0)\n",
    "  signal = np.pad(signal,((0,0),(L_all,0),(0,0)),constant_values=0.0)\n",
    "  signal = signal.reshape(-1, N_input + N_output)\n",
    "  scaled_signal = real_scaler.transform(signal)\n",
    "  scaled_signal = scaled_signal.reshape(-1, L_ground+L_all, N_input + N_output)\n",
    "  X_times_series, Y_label = prepare_data_batch(scaled_signal, L_all, L_out, list_output_time, j = sliding_step)\n",
    "  X_times_series[:,:,L_in:,N_input:N_input+N_output] = 0.0\n",
    "\n",
    "  static = np.stack(list(df_new[k][list_idx] for k in list_static)).T\n",
    "  scaled_static = static_scaler.transform(static)\n",
    "  X_static = scaled_static.reshape(-1,1,1,N_static)\n",
    "  X_static = np.tile(X_static, [1, X_times_series.shape[1], 1, 1])\n",
    "  X_times_series = X_times_series.reshape(-1, L_all, N_input+N_output)\n",
    "  X_static = X_static.reshape(-1, 1, N_static)\n",
    "  Y_label =  Y_label.reshape(-1, L_out, N_output)\n",
    "\n",
    "  return X_times_series, X_static, Y_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hr5y1saip31Y"
   },
   "outputs": [],
   "source": [
    "def FullAttention(queries, keys, values):\n",
    "      B, L, H, E = queries.shape\n",
    "      _, S, _, D = values.shape\n",
    "      scale = 1. / math.sqrt(E)\n",
    "      scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
    "      A = torch.softmax(scale * scores, dim=-1)\n",
    "      V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
    "      return V.contiguous()\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, dropout, n_heads=4):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        d_keys = d_model // n_heads\n",
    "        d_values = d_model // n_heads\n",
    "        self.attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, _ = x.shape\n",
    "        _, S, _ = x.shape\n",
    "        H = self.n_heads\n",
    "        queries = self.query_projection(x).view(B, L, H, -1)\n",
    "        keys = self.key_projection(x).view(B, S, H, -1)\n",
    "        values = self.value_projection(x).view(B, S, H, -1)\n",
    "        out = self.attention(queries,keys,values)\n",
    "        out = out.view(B, L, -1)\n",
    "        out = self.out_projection(out)\n",
    "        x = self.dropout(F.relu(x + out))\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = x.permute(0, 2, 1)  # Shape: [N_sample, N_timestep, d_model]\n",
    "        x = self.conv(x)  # Shape: [N_sample, d_model, N_timestep]\n",
    "        x = self.relu(x)\n",
    "        x = x.permute(0, 2, 1)  # Shape: [N_sample, d_model, N_timestep]\n",
    "        return x\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=d_model, hidden_size=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x, _ = self.lstm(x)  # Shape: [N_sample, N_timestep / 2, 64]\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(input_size=d_model, hidden_size=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = x.permute(0, 2, 1)  # Shape: [N_sample, N_timestep, d_model]\n",
    "        x = self.conv(x)  # Shape: [N_sample, d_model, N_timestep]\n",
    "        x = self.relu(x)\n",
    "        x = x.permute(0, 2, 1)  # Shape: [N_sample, d_model, N_timestep]\n",
    "        x, _ = self.lstm(x)  # Shape: [N_sample, N_timestep / 2, 64]\n",
    "        return x\n",
    "\n",
    "class Mamba_h_2(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(Mamba_h_2, self).__init__()\n",
    "        self.mamba = Mamba(\n",
    "            d_model = configs.d_model,\n",
    "            d_state = 16,\n",
    "            d_conv = 4,\n",
    "            expand = 2,\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mamba(x)\n",
    "        return x\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, configs, L_all, N_all):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "\n",
    "        self.temporal = nn.Sequential(\n",
    "            nn.Linear(L_all, configs.d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(configs.d_model, L_all),\n",
    "            nn.Dropout(configs.dropout)\n",
    "        )\n",
    "\n",
    "        self.channel = nn.Sequential(\n",
    "            nn.Linear(N_all, configs.d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(configs.d_model, N_all),\n",
    "            nn.Dropout(configs.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, L, D]\n",
    "        x = x + self.temporal(x.transpose(1, 2)).transpose(1, 2)\n",
    "        x = x + self.channel(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TSMixer_dis(nn.Module):\n",
    "    def __init__(self, configs, L_all, L_out, N_input, N_output, mtype='Mamba'):\n",
    "        super(TSMixer_dis, self).__init__()\n",
    "        self.layer = configs.e_layers\n",
    "        self.model = nn.ModuleList([])\n",
    "\n",
    "        if mtype == 'MLP':\n",
    "          self.model = nn.ModuleList([])\n",
    "        elif mtype == 'TSMixer':\n",
    "          self.model = nn.ModuleList([ResBlock(configs, L_all, N_input+N_output+1) for _ in range(configs.e_layers)])\n",
    "        elif mtype == 'TSMixer-embed':\n",
    "          self.model = nn.ModuleList([DataEmbedding(N_input+N_output+1, configs.d_model, configs.dropout)]\n",
    "                                    +[ResBlock(configs, L_all, configs.d_model) for _ in range(configs.e_layers)]\n",
    "                                    +[nn.Sequential(nn.Linear(configs.d_model, N_input+N_output+1), nn.ReLU())])\n",
    "        elif mtype == 'Transformer':\n",
    "          self.model = nn.ModuleList([DataEmbedding(N_input+N_output+1, configs.d_model, configs.dropout)]\n",
    "                                    +[AttentionLayer(FullAttention, configs.d_model, configs.dropout) for _ in range(configs.e_layers)]\n",
    "                                    +[nn.Sequential(nn.Linear(configs.d_model, N_input+N_output+1), nn.ReLU())])\n",
    "        elif mtype == 'CNNLSTM':\n",
    "          self.model = nn.ModuleList([DataEmbedding(N_input+N_output+1, configs.d_model, configs.dropout)]\n",
    "                                    +[CNNLSTM(configs.d_model) for _ in range(configs.e_layers)]\n",
    "                                    +[nn.Sequential(nn.Linear(configs.d_model, N_input+N_output+1), nn.ReLU())])\n",
    "        elif mtype == 'CNN':\n",
    "          self.model = nn.ModuleList([DataEmbedding(N_input+N_output+1, configs.d_model, configs.dropout)]\n",
    "                                    +[CNN(configs.d_model) for _ in range(configs.e_layers)]\n",
    "                                    +[nn.Sequential(nn.Linear(configs.d_model, N_input+N_output+1), nn.ReLU())])\n",
    "        elif mtype == 'LSTM':\n",
    "          self.model = nn.ModuleList([DataEmbedding(N_input+N_output+1, configs.d_model, configs.dropout)]\n",
    "                                    +[LSTM(configs.d_model) for _ in range(configs.e_layers)]\n",
    "                                    +[nn.Sequential(nn.Linear(configs.d_model, N_input+N_output+1), nn.ReLU())])\n",
    "        elif mtype == 'Mamba':\n",
    "          self.model = nn.ModuleList([DataEmbedding(N_input+N_output+1, configs.d_model, configs.dropout)]\n",
    "                                    +[Mamba_h_2(configs) for _ in range(configs.e_layers)]\n",
    "                                    +[nn.Sequential(nn.Linear(configs.d_model, N_input+N_output+1), nn.ReLU())])\n",
    "\n",
    "\n",
    "        self.projection = nn.Sequential(nn.Linear(L_all, L_out), nn.ReLU())\n",
    "        self.out_layer = nn.ModuleList([nn.Linear(N_input+N_output+1, N_output, bias=False) for i in range(7)])\n",
    "\n",
    "    def forward(self, x_enc):\n",
    "        x = x_enc\n",
    "        for i in range(len(self.model)):\n",
    "            x = self.model[i](x)\n",
    "\n",
    "        x = self.projection(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x_out = torch.stack([self.out_layer[i](x) for i in range(7)], dim=-1)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "class model_parameters:\n",
    "  def __init__(self):\n",
    "    self.d_model = 128\n",
    "    self.e_layers = 2\n",
    "    self.dropout = 0.1\n",
    "\n",
    "# [0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98],\n",
    "class MAE_2():\n",
    "    def loss(y_pred, target):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        L = torch.zeros(1).to(device)\n",
    "        for i, q in enumerate([0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]):\n",
    "          errors = y_pred[:,:,:,i] - target\n",
    "          L += torch.mean(torch.max((q - 1) * errors, q * errors))\n",
    "\n",
    "        return L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Grvcw3c8pib6"
   },
   "outputs": [],
   "source": [
    "def one_function_optim(ex_name, mtype='Mamba', N_epoch = 20, Pencode=True, verbose = 0):\n",
    "\n",
    "    d_model, e_layers, L_in, L_out, dropout = 128, 2, 50, 750, 0.1\n",
    "    L_all = L_in + L_out\n",
    "    sliding_step = L_out\n",
    "\n",
    "    with open(f'EQall.p','rb') as f:\n",
    "      EQall = pickle.load(f)\n",
    "\n",
    "    with open(f'new_p_data_allEQ_{ex_name}.p', 'rb') as f:\n",
    "      df_new = pickle.load(f)\n",
    "\n",
    "    for i in range(len(df_new)):\n",
    "      L = len(df_new['disp_ground'][i])\n",
    "      if L >3000:\n",
    "        df_new['disp_ground'][i] = df_new['disp_ground'][i][:3000]\n",
    "      else:\n",
    "        df_new['disp_ground'][i] = list(df_new['disp_ground'][i]) + [0]*(3000-L)\n",
    "\n",
    "    list_output_time = []\n",
    "    list_static = []\n",
    "\n",
    "    for k in list(df_new):\n",
    "      if (str(k)[:4] == 'disp') & (str(k)[-1] != 'd'):\n",
    "        list_output_time.append(k)\n",
    "\n",
    "      if str(k)[:4] != 'disp':\n",
    "        list_static.append(k)\n",
    "\n",
    "    df_new['EQid'] = list_EQid\n",
    "    list_len = np.array([len(df_new['disp_2'][ip]) for ip in range(len(df_new))])\n",
    "    list_idx = np.where(list_len!=3000)[0]\n",
    "    df_new = df_new.drop(index=list_idx)\n",
    "    df_new = df_new.reset_index()\n",
    "\n",
    "\n",
    "    L_signal_input = len(df_new['disp_ground'][0])\n",
    "    L_ground = L_signal_input\n",
    "\n",
    "    Ndim_encoding = 2\n",
    "    P_encoding = np.zeros((L_signal_input, Ndim_encoding))\n",
    "    if Pencode==True:\n",
    "      P_encoding = getPositionEncoding(seq_len=L_signal_input, d=Ndim_encoding, n=10000)\n",
    "\n",
    "    for d in range(Ndim_encoding):\n",
    "      df_new[f'encode_{d}'] = [P_encoding[:,d]]*len(df_new)\n",
    "\n",
    "    list_input_time = ['disp_ground']+[f'encode_{d}' for d in range(Ndim_encoding)]\n",
    "    list_time_series = list_input_time + list_output_time\n",
    "    L_in = L_all-L_out\n",
    "    N_input= len(list_input_time)\n",
    "    N_output= len(list_output_time)\n",
    "    N_static = len(list_static)\n",
    "    N_time_series = len(list_input_time)+len(list_output_time)+1\n",
    "\n",
    "    static_scaler = sklearn.preprocessing.StandardScaler().fit(df_new[list_static].values)\n",
    "    target = np.array([df_new[i][0] for i in list_output_time]).T\n",
    "    target_scaler = sklearn.preprocessing.StandardScaler().fit(target)\n",
    "    real = np.array([df_new[i][0] for i in list_time_series]).T\n",
    "    real_scaler = sklearn.preprocessing.StandardScaler().fit(real)\n",
    "\n",
    "\n",
    "    train_time, train_static, train_label = gen_data_AI_batch_2_model(df_new, idx_train, L_all, L_out, L_ground, N_input, N_output, N_static, sliding_step, list_time_series, list_output_time, list_static, real_scaler, static_scaler)\n",
    "    val_time, val_static, val_label = gen_data_AI_batch_2_model(df_new, idx_valid, L_all, L_out, L_ground, N_input, N_output, N_static, sliding_step, list_time_series, list_output_time, list_static, real_scaler, static_scaler)\n",
    "    _, train_loader = data_provider2(train_time, train_static, train_label, batch_size=64)\n",
    "    _, valid_loader = data_provider2(val_time, val_static, val_label, batch_size=64)\n",
    "\n",
    "\n",
    "    args = model_parameters()\n",
    "    args.d_model = d_model\n",
    "    args.e_layers = e_layers\n",
    "    args.dropout = dropout\n",
    "\n",
    "    model_tor = TSMixer_dis(args, L_all, L_out, N_input, N_output, mtype)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    aggregation = Data_aggregation(L_all, N_static, dropout)\n",
    "    aggregation = aggregation.float().to(device)\n",
    "    model_tor = model_tor.float().to(device)\n",
    "    model_optim = torch.optim.Adam(list(model_tor.parameters())+list(aggregation.parameters()), lr=0.001)\n",
    "    criterion = MAE_2\n",
    "\n",
    "    time1 = time.time()\n",
    "\n",
    "    for epoch in range(N_epoch):\n",
    "        train_loss = []\n",
    "        for batch_time, batch_static, batch_y in train_loader:\n",
    "            model_optim.zero_grad()\n",
    "            batch_time, batch_static, batch_y = batch_time.float().to(device), batch_static.float().to(device), batch_y.float().to(device)\n",
    "            batch_x = aggregation(batch_time, batch_static)\n",
    "            outputs = model_tor(batch_x)\n",
    "            loss = criterion.loss(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            train_loss.append(loss.item())\n",
    "            model_optim.step()\n",
    "\n",
    "        if verbose == 1:\n",
    "          if epoch % 2 ==0:\n",
    "            valid_loss = []\n",
    "            for batch_time, batch_static, batch_y in valid_loader:\n",
    "              batch_time, batch_static, batch_y = batch_time.float().to(device), batch_static.float().to(device), batch_y.float().to(device)\n",
    "              batch_x = aggregation(batch_time, batch_static)\n",
    "              outputs = model_tor(batch_x)\n",
    "              loss = criterion.loss(outputs, batch_y)\n",
    "              valid_loss.append(loss.item())\n",
    "\n",
    "            print(f\"epoch: {epoch} | loss: {np.mean(train_loss)}, valid loss: {np.mean(valid_loss)}\")\n",
    "\n",
    "\n",
    "    time2 = time.time()\n",
    "\n",
    "    list_idx = idx_valid\n",
    "    x_time, x_static, _ = gen_data_AI_batch_2_model(df_new, list_idx, L_all, L_out, L_ground, N_input, N_output, N_static, sliding_step, list_time_series, list_output_time, list_static, real_scaler, static_scaler)\n",
    "    x_time = x_time.reshape(len(list_idx),-1,L_all, N_input+N_output)\n",
    "    x_static = x_static.reshape(len(list_idx),-1, 1, N_static)\n",
    "\n",
    "    y_predict_all = np.zeros((len(x_time), L_ground+L_all, N_output))\n",
    "\n",
    "    for j in range(int(L_ground/L_out)+1):\n",
    "      x_time[:,j,:L_all, N_input:N_input+N_output] = y_predict_all[:,L_out*j:L_all+L_out*j,:]\n",
    "      xt = torch.from_numpy(x_time[:,j,:,:]).float().to('cuda')\n",
    "      xs = torch.from_numpy(x_static[:,j,:,:]).float().to('cuda')\n",
    "      batch_x = aggregation(xt, xs)\n",
    "      y_pred_all = model_tor(batch_x).to('cuda').cpu().detach().numpy()\n",
    "      y_predict_all[:,L_in+L_out*j:L_all+L_out*j,:] = y_pred_all[:,:,:,3]\n",
    "\n",
    "    y_predict_all = target_scaler.inverse_transform(y_predict_all.reshape(-1, N_output))\n",
    "    y_predict_all = y_predict_all.reshape(len(x_time), -1, N_output)\n",
    "\n",
    "\n",
    "    dict_MAE = {k:[] for k in list_output_time}\n",
    "    dict_MAPE = {k:[] for k in list_output_time}\n",
    "    dict_RMSE = {k:[] for k in list_output_time}\n",
    "    dict_R2 = {k:[] for k in list_output_time}\n",
    "\n",
    "    for i_s in range(len(list_output_time)):\n",
    "      for i in range(len(list_idx)):\n",
    "        calMAE = MAE(df_new[list_output_time[i_s]][list_idx[i]], y_predict_all[i, L_all:L_all+L_ground,i_s])\n",
    "        dict_MAE[list_output_time[i_s]].append(calMAE)\n",
    "        calR2 = R2(df_new[list_output_time[i_s]][list_idx[i]], y_predict_all[i, L_all:L_all+L_ground,i_s])\n",
    "        dict_R2[list_output_time[i_s]].append(calR2)\n",
    "\n",
    "        calMAPE = MAPE(df_new[list_output_time[i_s]][list_idx[i]], y_predict_all[i, L_all:L_all+L_ground,i_s])\n",
    "        dict_MAPE[list_output_time[i_s]].append(calMAPE)\n",
    "        calRMSE = RMSE(df_new[list_output_time[i_s]][list_idx[i]], y_predict_all[i, L_all:L_all+L_ground,i_s])\n",
    "        dict_RMSE[list_output_time[i_s]].append(calRMSE)\n",
    "\n",
    "\n",
    "\n",
    "    mean_MAE = [np.mean(dict_MAE[k]) for k in dict_MAE]\n",
    "    mean_MAPE = [np.mean(dict_MAPE[k]) for k in dict_MAPE]\n",
    "    mean_RMSE = [np.mean(dict_RMSE[k]) for k in dict_RMSE]\n",
    "    mean_R2 = [np.mean(dict_R2[k]) for k in dict_R2]\n",
    "\n",
    "    time3 = time.time()\n",
    "\n",
    "    total_params = sum(p.numel() for p in model_tor.parameters() if p.requires_grad)\n",
    "\n",
    "    print(summary(model_tor, batch_x.size()))\n",
    "\n",
    "    return mean_R2[-1], mean_MAE[-1], mean_MAPE[-1], mean_RMSE[-1], time1, time2, time3, total_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDPdjXL2ToSq"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "list_type = ['MLP', 'TSMixer', 'Transformer', 'CNNLSTM', 'CNN', 'LSTM', 'Mamba']\n",
    "list_ex = ['ex1', 'ex2', 'ex3', 'ex4', 'ex5', '3Dex1', '3Dex2', '3Dex3', '3Dex4', '3Dex5']\n",
    "for ex_name in list_ex:\n",
    "  if len(ex_name)>3:\n",
    "    Nex = 230\n",
    "  else:\n",
    "    Nex=250\n",
    "\n",
    "  idx_train = np.random.choice(Nex, size=200, replace=False)\n",
    "  idx_valid = [i for i in range(Nex) if i not in idx_train]\n",
    "\n",
    "  for mtype in list_type:\n",
    "    if 'noencode' in mtype:\n",
    "      Pencode = False\n",
    "    else:\n",
    "      Pencode = True\n",
    "\n",
    "    _, _, _, _, time1, time2, time3, total_params = one_function_optim(ex_name, mtype=mtype, N_epoch=1, Pencode = Pencode, verbose=0)\n",
    "    # print(ex_name, mtype, (time2-time1)*10, time3-time2)\n",
    "    print(ex_name, mtype, total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-J_NsHpGiaOH",
    "outputId": "bc7e69a4-ad7d-458e-9b0e-1366fac021a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaOkjRZWiYrA"
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vx4H32f7gepN",
    "outputId": "b8f60787-0aff-4886-f349-fa8ad132110b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TSMixer_dis                              [64, 750, 3, 7]           --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─DataEmbedding: 2-1                [64, 800, 128]            --\n",
       "│    │    └─TokenEmbedding: 3-1          [64, 800, 128]            2,688\n",
       "│    │    └─PositionalEmbedding: 3-2     [1, 800, 128]             --\n",
       "│    │    └─Dropout: 3-3                 [64, 800, 128]            --\n",
       "│    └─Mamba_h_2: 2-2                    [64, 800, 128]            --\n",
       "│    │    └─Mamba: 3-4                   [64, 800, 128]            116,480\n",
       "│    └─Mamba_h_2: 2-3                    [64, 800, 128]            --\n",
       "│    │    └─Mamba: 3-5                   [64, 800, 128]            116,480\n",
       "│    └─Sequential: 2-4                   [64, 800, 7]              --\n",
       "│    │    └─Linear: 3-6                  [64, 800, 7]              903\n",
       "│    │    └─ReLU: 3-7                    [64, 800, 7]              --\n",
       "├─Sequential: 1-2                        [64, 7, 750]              --\n",
       "│    └─Linear: 2-5                       [64, 7, 750]              600,750\n",
       "│    └─ReLU: 2-6                         [64, 7, 750]              --\n",
       "├─ModuleList: 1-3                        --                        --\n",
       "│    └─Linear: 2-7                       [64, 750, 3]              21\n",
       "│    └─Linear: 2-8                       [64, 750, 3]              21\n",
       "│    └─Linear: 2-9                       [64, 750, 3]              21\n",
       "│    └─Linear: 2-10                      [64, 750, 3]              21\n",
       "│    └─Linear: 2-11                      [64, 750, 3]              21\n",
       "│    └─Linear: 2-12                      [64, 750, 3]              21\n",
       "│    └─Linear: 2-13                      [64, 750, 3]              21\n",
       "==========================================================================================\n",
       "Total params: 837,448\n",
       "Trainable params: 837,448\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 176.14\n",
       "==========================================================================================\n",
       "Input size (MB): 1.43\n",
       "Forward/backward pass size (MB): 66.05\n",
       "Params size (MB): 2.42\n",
       "Estimated Total Size (MB): 69.90\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size=(64, 800, 7)\n",
    "summary(model_tor, input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOrYEjlupieh",
    "outputId": "b6ecbe06-7996-4335-f2ac-3b3641ce479f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3Dex1\n",
      "MLP -41.30167022929695 0.004930706913994626 54.86519013967485 0.007142784977770953\n",
      "3Dex1\n",
      "TSMixer -1.7081752377036175 0.002693610120664471 15.351447922294646 0.0038243283483156473\n",
      "3Dex1\n",
      "Transformer -1.9379068890026638 0.00367475644593396 227.83931493289438 0.005117187521910883\n",
      "3Dex1\n",
      "CNNLSTM -0.6536314208606203 0.0035777336646286104 15.974262502877327 0.0050171001055147\n",
      "3Dex1\n",
      "CNN -1.5152092340796561 0.004471504540059696 9.306769006012159 0.006507840409558719\n",
      "3Dex1\n",
      "LSTM -4.1013917679648 0.003958994948053806 20.181044928752318 0.005722930824058763\n",
      "3Dex1\n",
      "Mamba -64.1251116112397 0.006714151556961109 59.45187587026573 0.009292163314314582\n",
      "3Dex3\n",
      "MLP -0.39479537229373274 0.009075627643508277 62.9665461958797 0.013821717426227035\n",
      "3Dex3\n",
      "TSMixer 0.1321487538680185 0.0040206717331179595 68.36192657101164 0.00576866338784523\n",
      "3Dex3\n",
      "Transformer -0.25957298949196184 0.006737781973443727 87.09622548679822 0.009963915672502762\n",
      "3Dex3\n",
      "CNNLSTM -0.004354711770745668 0.01253231749863887 19.12086670495518 0.01822246808428873\n",
      "3Dex3\n",
      "CNN -0.5117664891819839 0.005357907490366727 58.74092586056285 0.007780293709506006\n",
      "3Dex3\n",
      "LSTM -0.43290475952630186 0.00547225264109143 70.7326364583363 0.007875025128195098\n",
      "3Dex3\n",
      "Mamba -0.004614044571300538 0.012532745764387001 26.02408412937817 0.01822262889072103\n",
      "3Dex4\n",
      "MLP -2.0926480253405244 0.01045962299126297 199.58391108369906 0.01612988214817202\n",
      "3Dex4\n",
      "TSMixer 0.7316703970988134 0.0037342305685448395 68.88947068117803 0.005320985279024478\n",
      "3Dex4\n",
      "Transformer -0.6502585513792816 0.01176519531838201 18.98199054010175 0.016573461088860052\n",
      "3Dex4\n",
      "CNNLSTM -0.0027369224593804644 0.013088108081780082 16.632067081580328 0.01814392401206279\n",
      "3Dex4\n",
      "CNN -0.8804564078341145 0.005244436920659992 22.75284286304132 0.007546254803777589\n",
      "3Dex4\n",
      "LSTM 0.3670431587434859 0.00559994121130373 22.239735259260115 0.007923392434730081\n",
      "3Dex4\n",
      "Mamba -0.1257318988391866 0.011371121565575192 25.033486521328093 0.0169396555691177\n",
      "3Dex6\n",
      "MLP 0.234438303459223 0.0017305285812613683 30.288992955271528 0.0028703762395128596\n",
      "3Dex6\n",
      "TSMixer 0.9553377069546704 0.00043698083295993134 50.43829969827802 0.000610612709317511\n",
      "3Dex6\n",
      "Transformer 0.7781150145814412 0.0008503290535986723 71.52332498948206 0.0011932382741744905\n",
      "3Dex6\n",
      "CNNLSTM 0.8210861190303932 0.0007827711444051026 17.79450886461156 0.0011336851812151255\n",
      "3Dex6\n",
      "CNN 0.8537819330224862 0.0006368096822241198 27.34542556373345 0.0008865450978113776\n",
      "3Dex6\n",
      "LSTM 0.8058328350252066 0.0007146971511132701 59.12264756455079 0.001014933876104969\n",
      "3Dex6\n",
      "Mamba 0.5316501101103382 0.0009760964229772877 112.5806643292923 0.0013782989506847124\n",
      "3Dex9\n",
      "MLP -1.2327703051761427 0.01091253261510861 107.61372403491008 0.01639742003091259\n",
      "3Dex9\n",
      "TSMixer 0.5974025738143294 0.0037165879079635988 35.204506977213626 0.0058243880243021035\n",
      "3Dex9\n",
      "Transformer -4.276554057618154 0.014876220856968188 71.85434605208765 0.02092555236118256\n",
      "3Dex9\n",
      "CNNLSTM -5.959898919510718 0.00659947971761864 8.63360340474227 0.00959139667645798\n",
      "3Dex9\n",
      "CNN -1.5624055935679853 0.0061033900498312504 31.029813884803698 0.00892708776271576\n",
      "3Dex9\n",
      "LSTM -0.5233211748147818 0.007353588521663938 14.774756001943413 0.010787645610691695\n",
      "3Dex9\n",
      "Mamba -0.20445161622383778 0.01345432345314575 91.74329110935095 0.01943738654589017\n",
      "CPU times: user 1h 52min, sys: 16.6 s, total: 1h 52min 17s\n",
      "Wall time: 1h 53min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "list_type = ['MLP', 'TSMixer', 'Transformer', 'CNNLSTM', 'CNN', 'LSTM', 'Mamba']\n",
    "list_ex = ['3Dex1', '3Dex2', '3Dex3', '3Dex4', '3Dex5']\n",
    "idx_train = np.random.choice(230, size=200, replace=False)\n",
    "idx_valid = [i for i in range(230) if i not in idx_train]\n",
    "\n",
    "for ex_name in list_ex:\n",
    "  for mtype in list_type:\n",
    "    value_R2, value_MAE, value_MAPE, value_RMSE = one_function_optim(ex_name, mtype=mtype, N_epoch=200, Pencode = True, verbose=0)\n",
    "    print(mtype, value_R2, value_MAE, value_MAPE, value_RMSE)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
